{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Datascrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In general, need to a bit more clear on what output should look like.\n",
    "\n",
    "In this problem set we are going to be developing our own custom data scrappers, use some other libraries to scrape data, and finally we will be utilizing **Amazon’s Web Services** to save data on the cloud. We will also create a `virtual machine` on **AWS** and continuously scrape data, without having to have our own computer on the whole time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s start by setting up an **AWS** account. **AWS** provides a number of cloud services that make it very easy to setup remote processes, and take advantage of cloud computing. Their built-in console facilitates the creation of instances of services such as remote servers, and web-based storage. We will be using **EC2** and **S3**. **EC2** allows us to create virtual servers, so we can run scheduled tasks without having to run them continuously on our computer. **S3** provides storage space on the cloud. We can easily save any scrapped data into a **bucket** or data container for future use.\n",
    "\n",
    "AWS offers a [free tier]( https://aws.amazon.com/free) of services. Most of what we will be using in class should be covered by the free tier of usage. Once we create an account, we can set up an **EC2** server, or an **S3** bucket. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build a scrapper for foursquare, and a scrapper for instagram. Lets install the libraries from the command line:\n",
    "\n",
    "```Python\n",
    "# Python library for accessing foursquare's API\n",
    "pip install foursquare\n",
    "\n",
    "# Python library for accessing instagram's API\n",
    "pip install python-instagram\n",
    "\n",
    "# Python library for interfacing with AWS\n",
    "pip install boto \n",
    "```\n",
    "\n",
    "> Add some more for setup, such as when where to create your AWS account, create a EC2 server, and create an S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Following the instructions found in the in-class tutorial, and python foursquare's [**API**](https://github.com/mLewisLogic/foursquare), create a scraper that continuously gets the trending venues in Riyadh. \n",
    "\n",
    "\n",
    "* First, make sure to [create your developer passwords]( https://foursquare.com/developers/apps) for foursquare. You will need them to authenticate.\n",
    "\n",
    "> Explain this just a bit more... what do you put for URLs and redirect URIs?\n",
    "\n",
    "* Second, look at [trending]( https://developer.foursquare.com/docs/venues/trending) end point of Foursquare’s **API**. Endpoints allow access to some of the resources from the API. More documentation can be accessed [here]( https://developer.foursquare.com/docs/). The python library provides an easy to use wrapped around the foursquare **API**, so we don’t need to come up with **REST** requests ourselves. \n",
    "\n",
    "* Third, create a python function that uses the python library, and authenticates you with the key, and hits the API returning a number of trending venues.\n",
    "    1. Inputs: \n",
    "      1. A `string` that represents a location in Riyadh. You might want to use a point close to the city center. \n",
    "      * Limit the search to 1,000 results, and a 5,000 mt. radius.\n",
    "    2. Outputs:\n",
    "      1. A `response dictionary` returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fourth, using [boto]( https://github.com/boto/boto), a python wrapper for **AWS**, create a python function that can upload a string to **AWS** [**S3**]( https://aws.amazon.com/s3/). S3 provides cloud buckets where you can store information such as `JSON`s or `strings`. First, you will need to [sign up]( https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?nc2=h_ct) for an account (you get some free services, and 5 gb of free storage).  Then, you will have to get [access keys]( http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html).\n",
    "\n",
    "**The function should:**\n",
    "* Create an [S3 connection]( http://docs.pythonboto.org/en/latest/ref/s3.html)\n",
    "* Select a bucket from your S3 account (You should previously [create]( http://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html) the S3 bucket). \n",
    "* Upload a string to a target path that is **specific** to that string. IE you can use the **date and time** as the path, to make sure no 2 files are the same. \n",
    "\n",
    "    1. Inputs: \n",
    "      1. A `string` that represents the target path to upload the string.\n",
    "      * A `string` which is a `json` containing the response from the endpoint.\n",
    "    2. Outputs:\n",
    "      1. A `printed` message of success if uploaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fifth, a function that consecutively hits foursquare’s **API** using the first function you built. The function should hit the API every `N` time, for a given period of time. You can use a library like [threading]( https://docs.python.org/2/library/threading.html) to repeat the operation for given period of time. The function should also parse the response into a dictionary, and keep the following keys:\n",
    "\n",
    "```Python\n",
    "                checkin['name']\n",
    "                checkin['hereNow']['count']\n",
    "                checkin['categories'][0]['name']\n",
    "                checkin['location']['lat']\n",
    "                checkin['location']['lng']\n",
    "                checkin['stats']['checkinsCount']\n",
    "                checkin['stats']['usersCount']\n",
    "                datetime\n",
    "                \n",
    "```\n",
    "\n",
    "1. Inputs: \n",
    "    1. A `number` that defines the total time to run the function for.\n",
    "2. Outputs:\n",
    "    1. A `json` of the parsed response.\n",
    "    2. Use your S3 function to upload it to your bucket. Add a screenshot of your bucket with the files to the ipython notebook.\n",
    "    \n",
    "    >removed second checkins count, are we using an ipython notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Following the instructions found in the in-class tutorial, and python instagram’s [**API**](https://github.com/Instagram/python-instagram), create a scraper that continuously gets the recent media in Riyadh. \n",
    "\n",
    "\n",
    "* First, make sure to [create your developer passwords](https://www.instagram.com/developer/) for Instagram. You will need them to authenticate.\n",
    "\n",
    "* Second, look at the [location](https://www.instagram.com/developer/endpoints/locations/) end points of Instagram’s **API**. \n",
    "\n",
    "* Third, authenticate into the instagram API. Then, create a python function that uses the python instagram library, and hits the API returning a number of recent media by location.\n",
    "    1. Inputs: \n",
    "      1. A `list` that contains a number of Instagram place id’s in Riyadh. \n",
    "    2. Outputs:\n",
    "      1. A `response` [**media**](https://github.com/Instagram/python-instagram/blob/master/instagram/models.py#L46) object.\n",
    "      \n",
    "      > What exactly are you looking for here? What is a media object? Do we have to use the Python Instagram library?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fourth, create a python function that uses the python instagram library, and hits the API to search for location id’s. You can use the `location_search` function of the library. This should return a list of places that you can then use to search for recent media.\n",
    "    1. Inputs: \n",
    "      1. Two location `float` numbers representing the lat and lon of Riyadh.\n",
    "      * Limit the search to 5,000 results, and a 5,000 mt. radius.\n",
    "    2. Outputs:\n",
    "      1. A `list` of Instagram locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fifth, a function that consecutively hitsinstagram’s **API** using the first function you built. The function should hit the API every `N` time, for a given period of time. You can use a library like [threading]( https://docs.python.org/2/library/threading.html) to repeat the operation for given period of time. The function should also parse the response object into a dictionary, and keep the keys that are relevant to you:\n",
    "\n",
    "1. Inputs: \n",
    "    1. A `number` that defines the total time to run the function for.\n",
    "    2. A `list` of locations that will be used to run your previous function.\n",
    "\n",
    "2. Outputs:\n",
    "    1. A `json` of the parsed response.\n",
    "    2. Use your S3 function to upload it to your bucket. Add a screenshot of your bucket with the files to the ipython notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Now, lets read some of the jsons we have been collecting, and create a quick plot based on their geo-location and some of their basic information.  \n",
    "\n",
    "\n",
    "* Download the foursquare files from your S3 bucket. You can use a UI like [S3 Browser](http://s3browser.com/), or write a Boto function.\n",
    "\n",
    "* Create a function that looks for all the files within a given local directory, and opens them. Here is some documentation: [list directory](https://docs.python.org/2/library/os.html#os.listdir), [open files](https://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files) \n",
    "\n",
    "* Third, plot the latitude and longitude of each **unique** venue. The radius of the circle should be proportional to the number of check-ins per venue. \n",
    "    1. Inputs: \n",
    "      1. A `string` that represents a file path containing all the jsons from S3. Alternatively, you can just write a function to download the files from S3 on real time by passing your credentials, and a bucket name.  \n",
    "    2. Outputs:\n",
    "      1. A plot with the unique venues represented by a scatter plot. The radius of each circle should be related to the number of check-ins per venue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
